# 2020年7月

## 2020/07/01

- [x] 组会

  定步长积分

  变步长积分：需要给定精度

  潘斌风 西工大 同伦方法

  二次同伦：同伦参数较大时，可以用变步长积分

  下周要读一些论文

- [ ] Leetcode

- [x] 机器学习——李宏毅

  **deep learning**

  define a set of function、goodness of function (loss function)、pick the best function

  Neural Network，parameter，weight、 bias

  Fully Connect Feed-forward Network

  input, hidden layer, output

  network structure: how many layers, neurons?

  **Back propagation**

  计算偏导数的方法

  Forward pass, Backward pass

- [ ] 引力辅助程序

  同伦参数为1.0时，剩余质量为18173.499

  二次同伦至0.085，剩余质量为18266.259

  **今天的计算结果**

  二次同伦至0.05，剩余质量为18271.756，代入程序计算成功

  二次同伦至0.045，剩余质量为18271.893，代入程序计算成功，后续剩余质量开始减小

  后续计算结果代入程序，无法收敛

  重新设计同伦过程：

  由于采用定步长积分，运行时间较长。

  如果从自己记录下来的中间过程的结果作为初值，可能会和从1.0开始的结果不同。

  从1.0到0.05，步长为0.005，0.085成功，剩余质量为18266.259，代入程序计算成功

  从0.085到0.05，步长为0.001，0.068成功，剩余质量为18269.525，代入程序计算成功

  从0.068到0.05，步长为0.0005，0.051成功，剩余质量为18271.700，代入程序计算成功
  
  从0.068到0.04，步长为0.0005，情况较为复杂：
  
  ​	0.0445时，剩余质量增大到最大，为18271.892，代入程序计算失败
  
  ​	向前寻找求解成功的结果，0.0485时，剩余质量为18271.824，代入程序计算成功
  
  从0.068到0.04，步长为0.0001
  
  ​	0.0479时，剩余质量为18271.845
  
  ​	0.0478时，剩余质量为18271.848
  
  ​	0.0473时，剩余质量为18271.862，代入程序计算成功
  
  ​	0.0469时，剩余质量为18271.871
  
  ​	0.0463时，剩余质量为18271.882
  
  ​	0.0459时，剩余质量为18271.887，代入程序计算成功
  
  ​	0.0443时，剩余质量为18271.891，代入程序计算成功
  
  ​	0.0436时，剩余质量为18271.883，**开始下降**，代入程序计算失败
  
  ​	0.0433时，剩余质量为18271.878
  
  ​	0.0415时，剩余质量为18271.827
  
  ​	0.0412时，剩余质量为18271.814
  
  ​	0.0407时，剩余质量为18271.791
  
  ​	0.0400时，剩余质量为18271.754
  
  总结：二次同伦同伦过程中，剩余质量逐渐增大，在同伦参数为0.0443时，剩余质量达到最大值，之后剩余质量开始减小。但是将剩余质量减小过程中的计算结果代入程序，计算失败。

## 2020/07/02

- [ ] 引力辅助程序

- [ ] 阅读文献 Fast preliminary design of low-thrust trajectories for multi-asteroid exploration

- [ ] 机器学习——李宏毅

  **CNN**

  Convolution、Max Pooling 两部分循环进行多次

  Convolution: filter size stride 一个filter检测一个特征 filter是训练出来的

  Max Pooling: 尺寸缩小

  Flatten

  神经网络

  **keras**

  batch size

## 2020/07/03

- [ ] 引力辅助程序

  * 请教武迪师兄

    武迪师兄建议：同伦过程中每一步都要是收敛解，会导致计算时间变长。

    调用新的积分函数之后，`x0`不会变化，最终积分结果是`xf`，所以输出最终质量时应该用`xf[6]`，如果仍然用`x0[6]`，输出的其实是引力辅助时的质量，因此同伦过程中没有呈现出一直增大的趋势。

    改正错误之后，重新进行同伦，每个同伦参数对应的打靶变量值保持不变，只有剩余质量的输出值发生改变。

    同伦参数为1.0，剩余质量为15735.957

    同伦参数为0.085，剩余质量为15999.912

    同伦参数为0.068，剩余质量为16005.793

    同伦参数为0.051，剩余质量为16011.347

    同伦参数为0.0485，剩余质量为16012.120

    同伦参数为0.0473，剩余质量为16012.486

    同伦参数为0.0459，剩余质量为16012.909

    同伦参数为0.0443，剩余质量为16013.388

    以上代入程序计算，均成功

    同伦参数为0.0436，剩余质量为16013.596，代入程序计算失败

    同伦参数为0.0401，剩余质量为16014.620，代入程序计算失败

    尝试增加打靶变量输出的精度，多输出几位小数。

    输出15位小数

    同伦参数为0.0436，剩余质量为16013.596，代入程序计算成功

    同伦参数为0.0401，剩余质量为16014.620，代入程序计算成功

    说明输出小数位数增加十分有效。

    重新开始同伦过程：

    同伦参数为0.0302，剩余质量为16017.362，代入程序计算成功

    同伦到0.0226，剩余质量为16019.185

  * 将对数同伦1.2e-5的初值，代入bang-bang控制，采用带开关函数检测的定步长积分，观察是否收敛

- [x] 机器学习——李宏毅

  **train 结果不好时**：new activation function/learning rate

  sigmoid 会导致input附近的参数对结果的影响很小

  ReLU/Maxout

  RMSProp

  Momentum 惯性：可以跳出局部最优

  Adam = RMSProp + Momentum

  **test 结果不好时**：overfitting

  early stopping

  Regularization L1 L2

  dropout

## 2020/07/04

- [ ] 引力辅助程序

  * 二次同伦

    步长为0.00001，同伦到0.02，剩余质量为16019.696

    步长为0.00001，同伦到0.01，剩余质量为16021.135

    步长为0.00001，同伦到0.001，剩余质量为16021.637

- [ ] 机器学习——李宏毅

  * PyTorch Tutorial

- [ ] Programming for Everybody

  * 1.1 Why Program
  * 1.2 Hardware Overview
  * 1.3 Python as a Language

## 2020/07/05

- [x] 引力辅助程序

  * 二次同伦

    步长为0.00001，同伦到0.00003，剩余质量为16021.641905406350000
    
  
- [x] Programming for Everybody

  * 1.4 Writing Paragraphs of Code

    reserved words

    sequential, conditional, repeated step

  * 2.1 Expressions

    Constants, Reserved words, variables

  * 2.2 Expressions Part 2

    Operator, Order, Type, `type()`

  * 2.3 Expression Part 3

    comments

## 2020/07/06

- [ ] 引力辅助程序

  * 编写间接法求解bang-bang控制的程序，采用带开关函数检测的定步长积分

  * 对数同伦

    将同伦参数1.2E-5对应的计算结果作为初值，求解bang-bang控制，不收敛，尝试更小的同伦参数。

    当同伦参数小于1.2E-5之后，改变同伦策略，每次同伦参数变为原来的1/2.

    同伦至2.3437500e-8，剩余质量为16021.641737147094000，代入原程序计算失败

    增加输出结果输出位数为20为，仍然失败。

    猜测原因：对数同伦到一定阶段，由于ode45积分精度的影响，导致计算失败。

    将ode45积分精度提高为1e-15，中间变为另一个局部最优解，将不等式约束中的乘子去掉。

    结果不好，仍然改回原来的状态。

    

    目前对数同伦所达到的最好结果为

    **同伦参数为2.343749999831972e-008，剩余质量为16021.641737147094000**

    但是，求解bang-bang控制，不收敛。

    估计还是因为对数同伦不容易同伦到bang-bang控制

  * 二次同伦

    同伦到3E-6，剩余质量为16021.641903042590000，代入程序计算成功，代入bang-bang控制，失败

    同伦到0.000000000083628，8.3628E-11，剩余质量为16021.641903668955000，代入程序计算成功
    
  * 改变计算精度，会导致同伦过程发生变化，目前没有统一的规律。

    精度包括：对数同伦ode45积分精度、定步长积分步长、hybrd精度要求等

- [ ] Python for Everybody

  * 3.1 Conditional Steps

  * 3.2 More Conditional Statements

    try/except, quit()

## 2020/07/07

- [x] 讨论班

  * 张众——区域目标可见窗口的快速预测方法

    计算效率

  * 张楠——TSP问题

    智能优化算法

- [x] 引力辅助程序

  - 在二次同伦过程最后，将`epsi`设置为0.0，运用二次同伦的打靶方程进行计算，成功收敛，将其作为初值代入二次同伦随机猜测函数`GA_FOP_type2()`中，可以成功收敛
  - 但是，无论是对数同伦，还是二次同伦，同伦到什么程度，积分步长为多少，计算结果代入专门为bang-bang控制编写的打靶函数`GA_FOP_type1()`中，都不收敛。

- [x] 阅读文献——预计三篇

## 2020/07/08

- [x] 组会
  * 研究方向
    * 中途飞越问题协态变量转换
    * 从Casalino文章出发，研究小推力轨迹优化中协态变量的物理含义
    * 智能优化方法用于小推力多目标优化问题

- [x] 机器学习——李宏毅

  完成CNN图片分类的程序。

  由于**GPU存储空间不足**，需要适当减小batch_size。

- [x] Getting Started with Python

  * 4.1 Using Functions

  * 4.2 Building Functions

  * 5.1 Loops and Iteration

    iteration variable

    break, continue

  * 5.2 Definite Loops

    for

## 2020/07/09

* Getting Started with Python

  * 5.3 Finding the Largest Value

  * 5.4 Loop Idioms

    Counting, Filtering, Search

    None, is

  * **已完成**

* 机器学习——李宏毅

  * Graph Neural Networks(GNN)

    * Why do we need GNN?

      Classification, Generation

      **数据之间存在某种联系**

      label缺失

      **graph的数据形式是什么样的？**

    * Spatial-based GNN

      convolution的推广

      过程：Aggregate, Readout

      Model: NN4G, DCNN, DGC, MoNET, GraphSAGE, **GAT**, GIN

    * Graph Signal Processing, Spectral-based GNN

      数据形式：$G=(V,E)$， adjacency matrix,  degree matrix

      相邻结点的能量差$\rightarrow$频率

      Filtering

      Model: ChebNet(Chebyshev polynomial), **GCN**

      **可以用于解决TSP**

  * Recurrent Neural Network RNN

    1-of-N encoding, 上下文有联系

    相同的输入，可能会导致不同的输出，与输入顺序有关。

    * Elamn Network, Jordan Network

    * Bi-direction RNN

    * Long Short-term Memory (LSTM)

      Input Gate, Output Gate, Forget Gate

      4 input, 1 output, 4 times of parameters

    * loss function: cross entropy

    * Gradient descent: BPTT, Back propagation through time

    * Clipping

    * GRU, simpler than LSTM

## 2020/07/10

### 机器学习——李宏毅

1. Recurrent Neural Network (RNN)

   * Many to Many (No Limitation)

     翻译、语音识别+翻译

   * Beyond Sequence

   * Attention-based Model

   * GAN, structured learning

   * 作业

2. Semi-supervised Learning

   * unlabeled data >> labeled data, U >> R

   * Transductive learning, Inductive learning, 是否使用testing data

   * the distribution of the unlabeled data, assumption

   * Generative Model **Soft label**

     依赖于初始值的选取

   * Low-density Separation 非黑即白——分界线很明显 **Hard label**

     * Self-training 不适用于Regression

     * Entropy-based Regularization
       $$
       L=\sum_{x^r} C(y^r,\hat{y}^r)+\lambda \sum_{x^u}{E(y^u)}
       $$

   * Smoothness Assumption

     close in a high density region $\rightarrow$ same label

     Cluster and then label

     Graph-based Approach, Graph Construction, KNN

   * Better Representation

3. Unsupervised Learning - Linear Methods

   * Clustering & Dimension Reduction/Distributed representation

     * K-means
     * Hierarchical Agglomerative Clustering (HAC)

     ---

     * Feature selection

     * **Principle component analysis** (PCA) - decorrelation

## 2020/07/11

### 机器学习——李宏毅

#### 14. Unsupervised Learning - Word Embedding

* 1-of-N Encoding, Word Class, Word Embedding

* Auto-encoder

* Count based, Prediction based - Sharing Parameters

> 无监督学习中，如果想要得到某些结论，就必须要做出某种假设。

#### 15. Unsupervised Learning - Neighbor Embedding

* Locally Linear Embedding - LLE

* Laplacian Eigenmaps

* T-distributed Stochastic Neighbor Embedding (**t-SNE**)

  KL divergence

## 2020/07/13

### GTOC4

* 单段求解第一颗小行星到第二颗小行星的飞越问题

  设置epsi=1.0，最大猜测次数为1000。
  
  * 采用GTOC4中的原始数据，最大推力为0.135N，比冲为3000s，无法收敛。原因可能是Lambert双脉冲转移无法用小推力实现，为验证这一想法，**提高最大推力/比冲**。

  * 将最大推力设置为0.5N，可以收敛，剩余质量为1443.912kg。但是，**lam0接近于一，其余七个协态初值都接近为零**。之后，不考虑更大的推力。

  * 将最大推力设置为0.3N，很快收敛，剩余质量为1451.082kg，打靶变量的计算结果比较正常。

  * 将最大推力设置为0.2N，很快收敛，剩余质量为1449.687kg，打靶变量的计算结果比较正常。

  * 将最大推力设置为0.15N，无法收敛。

* 重新规划程序

  基本可以确认，单段求解无法收敛的原因是，推力太小，Lambert双脉冲转移无法用小推力实现。

  因此，在集束搜索过程中，需要**加强约束条件**，将$\Delta V_L < c\frac{T_{max}}{m_s}\Delta t$中的$c$由1.0改为0.7，重新进行集束搜索。

### 阅读文献

* Design space pruning techniques for low-thrust, multiple asteroid rendezvous trajectory design

## 2020/07/14

### 阅读文献

* Generic smoothing for optimal bang-off-bang spacecraft maneuvers

* Survey of numerical methods for trajectory optimization

## 2020/07/15

### GTOC4

* 集束搜索完成，运行时间约34小时。

  参数设置：集束宽度为20000，KNN=500
  
  运行结果：飞越小行星数量为44颗，总飞行时间为3640天，剩余质量为518.085 kg。

  约束条件加强，飞越数量反而增加。可以看出，之前的约束条件下，有一些有潜力的解被剔除了。

  间接法求解第一颗星到第二颗星的飞越问题，很快收敛成功，剩余质量为1461.093 kg。

  下面考虑求解后续各段，需要获取前一阶段的剩余质量和末端速度，采用之前输出剩余质量的方法，写入到`fvec`中，然后再读取到`Out`中。

  **以上所有结果全部是在最大推力为0.2 N的情况下计算出来的，不具有参考价值，需要重新进行计算。**

* 将推力改为0.135 N，$c$取0.7，飞越小行星的数量为0。

* $c$取1.0，验证程序本身是否出现问题。

## 2020/07/16

### 机器学习——李宏毅

#### 16. Unsupervised Learning - Auto-encoder

> 将输入量进行编码，得到维数更小的code；将code进行解码，得到原来的输入量。

由于我们并不知道code的值，上面两个部分无法单独训练；但是可以连接起来，一起学习。

PCA其实就是一个单层的Auto-encoder。

> **Deep Auto-encoder**：编码和解码过程都是多层的神经网络。

编码、解码过程不一定是**互逆**的，不需要对w和b进行这样的限定。

* 应用：文字处理

* 应用：以图找图，寻找code相似的图片

* 应用：Pre-training DNN，参数初始化

  把每一个中间层的结果，当作Auto-encoder的code，要求它能够解码得到原本的输入量，将学习出合适的参数作为初值。

  这种方法面临一个问题：**隐藏层的维数>输入量的维数**，需要加上一个很强的regularization才能解决。

  得到初值之后，进行find-tune简单的调整即可。

* 改进：De-noising auto-encoder

  x -> x' -> code -> y

  在输入量的基础上加入噪声。通过训练，让y和x最接近，更加robust。

* 改进：constrictive auto-encoder

  加入限制条件：input变化时，对code的影响较小。

* Auto-encoder for CNN

  convolution -> pooling -> convolution -> pooling

  将上面的过程看作编码过程，那么解码过程应该怎么构造？

  * Unpooling

    方法1：在pooling时，需要记录选取最大值的位置，unpooling时其他位置补0。

    方法2：将每个值对应的pooling前的那些位置都设置成相同的值。

  * Deconvolution

    deconvolution的过程其实就是convolution，只不过需要padding。
  
* Sequence-to-sequence Auto-encoder

* 随机给定code，可以使用decode解码过程来生成图片

#### 17. Unsupervised Learning - Deep Generative Model - I

> Generaion

* PixelRNN

  学习前面的像素点，从而生成新的像素点。

  用上半张图生成下半张图。

  从头开始创造，开始时需要加入随机因素。

* Variational Autoencoder (VAE)

  将Auto-encoder中解码的部分拿出来，随机给定code，生成图片。

  VAE：在Auto-encoder训练时，加入一些其他过程。

  从结果中可以解读出，code的每一维都代表什么具体的含义，从而给定特殊的code，产生想要的图片。

* Generative Adversarial Network (GAN)

#### 18. Unsupervised Learning - Deep Generative Model - II

* VAE vs auto-encoder

  VAE：在code上加上noise，但是noise的variance

  Gussian mixture Model, EM algorithm

  **Estimate the probability distribution**

  code不再是一个具体的vector，而是一个正态分布。对于给定的输入$x$，当我们从分布$q(z|x)$采样$z$然后从分布$p(x|z)$采样$\hat{x}$时，我们希望最大化$\hat{x}=x$的概率$P(x)$。

  **KL散度**

  Conditional VAE：根据code的某一个特性，画出具有这个特性的一系列图片。

  VAE存在的问题：无法产生新的图片，只是和训练集中的图片接近。

* Generative Adversarial Network (GAN) **Decoder**

  Generator vs Discriminator (Real images)

  Generator从来没有看过真正的图片，只是为了骗过Discriminator。

  **图片 -> Discriminator (NN) -> 0/1** 分类问题

  **vecotr -> Generator (NN) -> 图片** 通过训练，使得产生的图片可以骗过上一代的Discriminator。

  训练Generator的时候，必须把Discriminator的参数固定住。

  没有明确的指标判断生成的图片好不好。

## 2020/07/17

### 机器学习——李宏毅

#### 19. Transefer Learning

没有直接相关的data，或者相关的data很少。收集到的data可能是这样的：

* input domain 输入内容不同

* task 任务目标不同

Target Data, Source Data; labelled, unlabeled

* Target Data, Source Data都有label

  方法1：
  
  Model Fine-tuning：用Source Data训练一个模型，再用Target Data进行修正。

  技巧：
  
  Conservative Training 加入新的约束，修正之后不要变化太大

  Layer Transfer：保留其中的某些层，只修正其他层的参数。对于不同的task，保留的层不同，语音识别通常保留最后几层，图片识别通常保留前面几层。

  方法2：

  Multitask Learning：同样的data处理不同的task，有共通的地方，可以共用某几层神经网络，如多种语言的语音识别。

  改进：如何判断两个task能不能进行迁移？

  Progressive NN：后面的task可以借用前面task的参数，可以实现不改变每个task。

* unlabeled Target Data, labelled Source Data

  把Source Data当作训练集，Target Data当作测试集。

  方法1：
  
  Domain-adversarial training

  训练domain classifier和label predictor，目标是无法判断图片来自Target Data还是Source Data，同时正确预测数字。

  方法2：
  
  Zero-shot Learning

  Target Data和Source Data的任务目标不同

  不去具体分类图片属于哪个类别，而是判断图片里面有没有某些特征。

  改进：

  Convex Combination of Semantic Embedding 不需要训练

* Source Data没有label，Target Data有label

  方法：Self-taught learning

  > semi-supervised learning中的数据比较相关，迁移学习中关系不大

  在Source data上训练出feature extractor，在Target Data上进行调整。

* 都没有label

  方法：Self-taught Clustring

#### 20. Support Vector Machine

两个特色：Hinge Loss 和 Kernel Method

* 回顾Binary Classification

  Loss function：Sigmoid + cross entropy

* 新的Loss function：Hinge Loss

  $$
  l(f(x^n),y^n)=max(0,1-\hat{y}^n f(x^n))
  $$

  目标是使$\hat{y}^nf(x)>1$。

* Linear SVM

  线性函数模型w b

  损失函数：Hinge Loss + Regularization，是**凸函数**。

  二次规划问题，可以用梯度下降进行训练。

* Dual Representation

  训练出来的w其实是x的线性组合，系数不为零的x就是support vector。

  $$
  w = X\alpha \\
  f(x) = w^T x = \alpha^T X^T x = \Sigma_n \alpha_n (x^n \cdot x) \\
  f(x) = \Sigma_n \alpha_n K(x^n,x)
  $$

  需要找到一组最好的$\alpha$，让Loss function最小。

* Kernel Trick

  feature transform之后的内积，相当于执行一个特殊的K函数，减少运算量。

  RBF Kernel, Sigmoid Kernel

  那么，在应用的时候，kernel函数应该选什么形式？描述similarity

  适用于输入量不容易用vector描述的情形。

* SVM和Deep learning的比较

## 2020/07/20

### 机器学习——李宏毅

#### Explainable ML

机器学习在训练之后，可以根据给定的输入，得到一个结果，我们还想要它解释为什么得到这样的结果。

**Local Explanation** 针对这一个输入进行解释

**Global Explanation** 对一类输入进行解释

我们不可能完全知道机器学习是怎样工作的。

**Interpretable** vs **Powersful**

linear model, deep network

* Local Explanation

  依次对每个分量进行改动，判断输入的各个分量中，哪些重要，哪些不重要。

  Saliency map

  Gradient Saturation

  Attack Interpretation

* Global Explanation

  Activation Maximization 生成一组新的输入值，使得某一个输出最大

  $$
  x^* = arg \max_x y_i \\
  x^* = arg \max_x y_i + R(x) \\
  R(x) = -\sum_{i,j} |x_{ij}|
  $$

  * Generator (GAN, VAE)

    z -> Generator -> x -> classifier -> y

    不再使用x，而是找一个低维度的z，使得y最大，生成的图片更加正常
  
* 用一个model解释另一个model

  训练一个简单的模型(linear)，去模仿另一个复杂的模型的行为。

  可以只分析复杂模型的一部分。

  * **Local Interpretable Model-Agnostic Explanations** (LIME) 需要在目标输入量的附近取样

    图片先进行分区，随机丢弃一些分区，作为取样，得到复杂模型的输出，再找到类似结果的简单**线性模型**。

    线性模型的系数为零对应的分区起的作用较小，正系数对应积极作用，负系数对应反作用。

  * 用**决策树**作为简单模型，来模拟复杂模型。

    决策树不要太复杂。

    可以在训练神经网络的时候，就考虑到决策树的复杂度不要太高。

## 2020/07/27

### 回顾文献

* Designing complex interplanetary trajectories for the Global Trajectory Optimization competitions

	phasing value和orbit indicator的一致性可能只适用于多目标交会问题。而对于GTOC4，我只计算了Lambert转移第一段速度增量和orbit indicator的一致性，但没有使用多目标优化方法计算更有意义的phasing value。

	更深层次的问题在于：**对于飞越问题，应该如何定义一个合适的phasing value？**

	在集束搜索的过程中，本文首先通过orbit indicator找到BF颗“距离”最近的小行星，然后求解时间最优问题得到最短时间，在此基础上，对时间进行离散，选取l个时间点，求解燃料最优问题。

	由于该问题从某一颗小行星上出发，因此涉及到**选取初始天体**的问题，结果表明：从最大的cluster中选取初始天体，相较于随机选取初始天体，可以得到更好的结果。

## 2020/07/31

### 回顾文献

* A Tabu Search Methodology for Spacecraft Tour Trajectory Optimization

	Genaral Model: initial conditions(time, position, velocity), target objects(intercept/rendezvous), trajectory segments(continous decision variables: duration)

	Spacecraft Tour Trajectories: dynamics, finite burn, impulsive, impulsive to finite burn, Gravity Loss

	* 假设**小推力的推进模式**为：在一段轨迹中，**中间的一段连续推力用于实现飞越，末端的一段连续推力用于实现交会**。推力大小固定为最大值，推力方向用两个角度描述，两个角度均为时间的二次函数。通过选择合适的连续变量，来满足这段轨迹末端的飞越/交会要求。

	* 假设**脉冲推力的推进模式**为：在一段轨迹中，**中间的某个时刻施加脉冲用于实现飞越，末端施加脉冲用于实现交会**。相比于一般的Lambert转移，施加第一个脉冲的时间可以在中途的任意时间，因此**增加了一个时间变量**。但在文章后面的数值仿真中，始终假设第一个脉冲在初始时刻施加。

	* 如何判断脉冲推力可以用小推力实现：假设一直保持最大推力，计算连续推力所需的时间。以中间脉冲的时刻为中点，施加一段连续推力；以末端时刻为终点，施加一段连续推力。需要满足的约束是：**施加连续推力的时间段不能重叠**。

	* Gravity Loss 对**燃料消耗**和**连续推力时间**进行修正。

	**根据脉冲推力结果推算连续推力时间的两种方式**：
	
	1. 用速度增量通过火箭公式计算得到的燃料消耗计算推力时间
	
	2. 用速度增量计算动量变化量/冲量，直接计算推力时间，**这种约束更强**

	为了应用**best-first search**，必须定义新的目标函数，可以用来给序列长度不同的部分解进行排序。
	
	在**集束搜索**中同样需要定义新的目标函数，但部分解的序列长度相同，只需要比较剩余质量和剩余时间。

	参考文献：55 20

### GTOC4

* 初步筛选指标：improved orbit indicator

* 进一步筛选指标：Lambert转移第一段速度增量

* 判断能否用小推力实现，**禁忌搜索博士论文中的修正方法**，**作用时间不重叠**

* 部分解排序指标：Izzo文章有针对交会问题的比较，或者参考冠军队伍/博士论文中更复杂的筛选指标

	修改CNodeLeg中nodeIndex的定义，表示剩余燃料占总燃料的百分比 + 剩余时间占总时间的百分比。

	考虑剩余质量随时间线性变化，作为惩罚项